\section{Frontend}
The evaluation of the Elephant browser and the NetInfService applications tries to answer the following two core questions:

\begin{enumerate}
\item How much uplink bandwidth is saved?
\item How much of content is used multiple times throughout web pages?
\item How much of linked resources are dynamically generated?
\item How much time does it take to retrieve web page?
\end{enumerate}

\subsection{Test Setup}

The first test setup consists of a set of web pages from which each of four Android phones will automatically retrieve 15 in a random order. Using the logging functionality of the applications, information about how (Internet, Bluetooth, NRS or Database) resources are retrieved, how many bytes each resource consists of and how long it take to retrieve are acquired. The results are meant to give an idea of the answer to questions 1 and 2.

The web page sets are of sizes 15, 20, 25, 30 and 35. They are derived from the service Alexa \cite{alexa}, which is renowned for its web metrics. This service keeps track of the most visited web sites by country, and the top sites were used to create the sets.

For the backend the setup for the tests consist of a Name Resolution Service that is reset between each test.

The second test setup consisted of first retrieving all web pages from the set of 30 using one phone, and then repeat. The results are meant to give an idea of the answer to question 3.

\subsection{Hardware}

Tests are run on three Samsung Galaxy Nexus phones and one HTC One X phone using Android OS 4.1.1 Jellybean that were provided by Ericsson.

For the backend the Name Resolution Service was run on a Intel Core 2 Quad CPU Q9400 @ 2.66GHz Ã— 4 with 4 gigabytes of memory using Ubuntu 12.04 LTS.

\subsection{Limitations}

The backend Name Resolution Service supports two types of databases to use for storing published NDOs. The first uses Erlang lists stored in main memory, the other uses a Riak database. The list database was chosen for this test as it is more well tested and easier to work with.

This however means that the test is limited to using the free main memory of the system. A preliminary test using a set of 50 web pages caused the system to run out of memory, resulting in a crash. Therefore, the size of the sets are limited to a maximum of 35 web pages.

\subsubsection{Results}
% Plot of usage

% Table comparing time of access to each resource

% Table (or plot) of re-usage after period

\subsubsection{Conclusions}

From Figure TODO REF we can see that even without precaching and even when the phones are not downloading exactly the same web pages we still have a decrease of approximately 30\% when it comes to data retrieved from the Internet. With precaching of popular web pages this ratio should be possible to increase even further.

In Figure TODO REF it can also be seen that approximately TODO\% of resources are retrieved from the database. This is because these resources are linked to several times throughout the . Since the resource is cached in the database the first time, additional requests can use the database.

From figure TODO REF we can see that when accessing a web page a second time approximately TODO\% still has to be retrieved using the Internet. An example of when this happens is when a web pages links to a resource using javascript to add a timestamp to the resource's URL. The dynamic nature of this content means that it will not be in any cache when requested. This means there will be a certain amount of resource that will have to be retrieved from the Internet.

From the tests we can conclude that the retrieval of web pages is very slow. So slow that using this application in a real life situation probably only makes sense if there is no connection at all otherwise. The main culprit behind the long retrieval times seems to be the search. Since a search is done for every web resource the time quickly adds up. To improve the retrieval times of web pages the search needs to be optimized.